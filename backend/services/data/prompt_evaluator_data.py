from pydantic import BaseModel, Field
from typing import Optional, Dict, Any
from datetime import datetime

class PromptEvaluator(BaseModel):
    id: Optional[str] = Field(
        default=None,
        alias="_id",
        description="Unique identifier for the evaluation (auto-generated by MongoDB)",
        examples=["62c123456789abcdef123456"]
    )
    prompt: str = Field(
        description="The user prompt that is being evaluated",
        examples=["Create a snake game in Python."]
    )
    evaluation_method: str = Field(
        ...,
        description="The evaluation method used (e.g., 'human' or 'llm')",
        examples=["evaluator_human", "evaluator_llm"]
    )
    model: str = Field(
        description="The AI model used for evaluation",
        examples=["gpt-3.5-turbo", "gpt-4o"]
    )
    parsed_result: Dict[str, Any] = Field(
        description="The evaluation result parsed as a JSON object",
        examples=[{"prompt_rating": 7, "reasons": ["Clear objective", "Detailed instructions"]}]
    )
    created_at: datetime = Field(
        default_factory=datetime.utcnow,
        description="Timestamp when the evaluation was created"
    )
    updated_at: datetime = Field(
        default_factory=datetime.utcnow,
        description="Timestamp when the evaluation was last updated"
    )
    is_deleted: bool = Field(
        default=False,
        description="Flag indicating whether the evaluation is deleted"
    )

    class Config:
        populate_by_name = True
        from_attributes = True
